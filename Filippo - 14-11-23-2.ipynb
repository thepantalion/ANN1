{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T19:09:30.849639Z","iopub.status.busy":"2023-11-13T19:09:30.849257Z","iopub.status.idle":"2023-11-13T19:09:30.856119Z","shell.execute_reply":"2023-11-13T19:09:30.855220Z","shell.execute_reply.started":"2023-11-13T19:09:30.849608Z"},"trusted":true},"outputs":[],"source":["# Fix randomness and hide warnings\n","seed = 42\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","import numpy as np\n","np.random.seed(seed)\n","\n","import logging\n","import platform\n","\n","from datetime import datetime\n","\n","import random\n","random.seed(seed)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T19:09:31.875522Z","iopub.status.busy":"2023-11-13T19:09:31.875160Z","iopub.status.idle":"2023-11-13T19:09:31.885355Z","shell.execute_reply":"2023-11-13T19:09:31.884438Z","shell.execute_reply.started":"2023-11-13T19:09:31.875493Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.14.0\n"]}],"source":["# Import tensorflow\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","from tensorflow.keras import mixed_precision\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T19:09:32.424413Z","iopub.status.busy":"2023-11-13T19:09:32.423809Z","iopub.status.idle":"2023-11-13T19:09:32.429764Z","shell.execute_reply":"2023-11-13T19:09:32.428818Z","shell.execute_reply.started":"2023-11-13T19:09:32.424379Z"},"trusted":true},"outputs":[],"source":["# Import other libraries\n","import cv2\n","from tensorflow.keras.applications.mobilenet import preprocess_input\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","import seaborn as sns\n","from PIL import Image\n","import imagehash\n","from sklearn.model_selection import KFold\n","import zipfile"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Setup Mixed Precision\n","\n","# Detect TPU\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","except ValueError:\n","  tpu = None\n","\n","if tpu:\n","  policyConfig = 'mixed_bfloat16'\n","else: \n","  policyConfig = 'mixed_float16'\n","\n","#policy = tf.keras.mixed_precision.Policy(policyConfig)\n","#tf.keras.mixed_precision.set_global_policy(policy)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["### Load the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Load the data from the file\n","file_path = 'cleaned_data.npz'\n","loaded_data = np.load(file_path, allow_pickle=True)\n","\n","# Put data into proper arrays\n","data = loaded_data['data']\n","labels = loaded_data['labels']"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare and augment data"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T19:11:15.673161Z","iopub.status.busy":"2023-11-13T19:11:15.672652Z","iopub.status.idle":"2023-11-13T19:11:16.257852Z","shell.execute_reply":"2023-11-13T19:11:16.256893Z","shell.execute_reply.started":"2023-11-13T19:11:15.673108Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3060\n","1790\n"]}],"source":["healthy = []\n","unhealthy = []\n","\n","# Iterate through the images to separate them into the proper set\n","for i, image in enumerate(data):\n","    if labels[i] == 'unhealthy':\n","        unhealthy.append(image)\n","    else:\n","        healthy.append(image)\n","\n","print(len(healthy))\n","print(len(unhealthy))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["flip = tf.keras.Sequential([\n","  tfkl.RandomFlip(\"vertical\"),\n","])\n","\n","rotation = tf.keras.Sequential([\n","  tfkl.RandomRotation(0.2),\n","])\n","\n","zoom = tf.keras.Sequential([\n","  tfkl.RandomZoom(0.2),\n","])\n","\n","translation = tf.keras.Sequential([\n","  tfkl.RandomTranslation(0.2,0.2),\n","])\n","\n","contrast = tf.keras.Sequential([\n","  tfkl.RandomContrast(0.75),\n","])"]},{"cell_type":"markdown","metadata":{},"source":["####"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T19:11:16.260134Z","iopub.status.busy":"2023-11-13T19:11:16.259277Z","iopub.status.idle":"2023-11-13T19:11:22.110584Z","shell.execute_reply":"2023-11-13T19:11:22.109510Z","shell.execute_reply.started":"2023-11-13T19:11:16.260093Z"},"trusted":true},"outputs":[],"source":["healthy = np.array(healthy)\n","N = 900\n","N2 = int(N/2)\n","\n","# Preprocess 'healthy' set\n","preprocessed_healthy_flip = healthy[:N]\n","preprocessed_healthy_flip = flip(preprocessed_healthy_flip)\n","\n","preprocessed_healthy_rotate = healthy[:N]\n","preprocessed_healthy_rotate = rotation(preprocessed_healthy_rotate)\n","\n","preprocessed_healthy_zoom = healthy[: N2]\n","preprocessed_healthy_zoom = zoom(preprocessed_healthy_zoom)\n","\n","preprocessed_healthy_translation = healthy[: N2]\n","preprocessed_healthy_translation = translation(preprocessed_healthy_translation)\n","\n","preprocessed_healthy_constrast = healthy[: N2]\n","preprocessed_healthy_constrast = contrast(preprocessed_healthy_constrast)\n","\n","preprocessed_healthy = np.concatenate([\n","    preprocessed_healthy_rotate, \n","    preprocessed_healthy_flip,\n","    preprocessed_healthy_zoom,\n","    preprocessed_healthy_translation,\n","    preprocessed_healthy_constrast\n","], axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unhealthy = np.array(unhealthy)\n","M = 1500\n","M2 = int(M/2)\n","\n","# Preprocess 'healthy' set\n","preprocessed_unhealthy_flip = unhealthy[:M]\n","preprocessed_unhealthy_flip = flip(preprocessed_unhealthy_flip)\n","\n","preprocessed_unhealthy_rotate = unhealthy[:M]\n","preprocessed_unhealthy_rotate = rotation(preprocessed_unhealthy_rotate)\n","\n","preprocessed_unhealthy_zoom = unhealthy[: M2]\n","preprocessed_unhealthy_zoom = zoom(preprocessed_unhealthy_zoom)\n","\n","preprocessed_unhealthy_translation = unhealthy[: M2]\n","preprocessed_unhealthy_translation = translation(preprocessed_unhealthy_translation)\n","\n","preprocessed_unhealthy_constrast = unhealthy[: M2]\n","preprocessed_unhealthy_constrast = contrast(preprocessed_unhealthy_constrast)\n","\n","preprocessed_unhealthy = np.concatenate([\n","    preprocessed_unhealthy_rotate, \n","    preprocessed_unhealthy_flip,\n","    preprocessed_unhealthy_zoom,\n","    preprocessed_unhealthy_translation,\n","    preprocessed_unhealthy_constrast\n","], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate the preprocessed sets\n","preprocessed_data = np.concatenate([preprocessed_healthy, preprocessed_unhealthy], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot\n","healthy_np = np.array(healthy)\n","preprocessed_healthy_rotate_np = np.array(preprocessed_healthy_rotate)\n","preprocessed_healthy_flip_np = np.array(preprocessed_healthy_flip)\n","preprocessed_healthy_zoom_np = np.array(preprocessed_healthy_zoom)\n","preprocessed_healthy_translation_np = np.array(preprocessed_healthy_translation)\n","preprocessed_healthy_contrast_np = np.array(preprocessed_healthy_constrast)\n","\n","num_img = 10\n","\n","fig, axes = plt.subplots(3, num_img, figsize=(20, 4))\n","for i in range(num_img):\n","    ax = axes[0, i % num_img]\n","    ax.imshow(healthy_np[i].astype(np.uint8), cmap='gray')\n","    ax = axes[1, i % num_img]\n","    ax.imshow(preprocessed_healthy_rotate_np[i].astype(np.uint8), cmap='gray')\n","    ax = axes[2, i % num_img]\n","    ax.imshow(preprocessed_healthy_flip_np[i].astype(np.uint8), cmap='gray')\n","    ax = axes[1, i % num_img]\n","    ax.imshow(preprocessed_healthy_zoom_np[i].astype(np.uint8), cmap='gray')\n","    ax = axes[1, i % num_img]\n","    ax.imshow(preprocessed_healthy_translation_np[i].astype(np.uint8), cmap='gray')\n","    ax = axes[1, i % num_img]\n","    ax.imshow(preprocessed_healthy_contrast_np[i].astype(np.uint8), cmap='gray')\n","    \n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T19:11:22.112170Z","iopub.status.busy":"2023-11-13T19:11:22.111861Z","iopub.status.idle":"2023-11-13T19:11:22.118849Z","shell.execute_reply":"2023-11-13T19:11:22.117951Z","shell.execute_reply.started":"2023-11-13T19:11:22.112142Z"},"trusted":true},"outputs":[],"source":["print(len(preprocessed_healthy), len(healthy), len(preprocessed_healthy) + len(healthy), len(preprocessed_healthy_rotate), len(preprocessed_healthy2_flip))\n","print(len(preprocessed_unhealthy), len(unhealthy), len(preprocessed_unhealthy) + len(unhealthy), len(preprocessed_unhealthy1), len(preprocessed_unhealthy2))\n","print(len(preprocessed_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T19:11:27.368429Z","iopub.status.busy":"2023-11-13T19:11:27.368053Z","iopub.status.idle":"2023-11-13T19:11:30.153681Z","shell.execute_reply":"2023-11-13T19:11:30.152728Z","shell.execute_reply.started":"2023-11-13T19:11:27.368399Z"},"trusted":true},"outputs":[],"source":["augmented_data = np.concatenate([preprocessed_data, data], axis = 0)\n","\n","augmented_labels_healthy = np.full((2 * N,), 'healthy', dtype='object')\n","augmented_labels_unhealthy = np.full((2 * M,), 'unhealthy', dtype='object')\n","\n","augmented_labels = np.concatenate([augmented_labels_healthy, augmented_labels_unhealthy, labels], axis=0)\n","\n","np.savez('augmented_data.npz', data=augmented_data, labels=augmented_labels)\n","\n","print(len(augmented_data))\n","print(len(augmented_labels))"]},{"cell_type":"markdown","metadata":{},"source":["### Load and process the (augmented) dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:19:30.254927Z","iopub.status.busy":"2023-11-13T22:19:30.254521Z","iopub.status.idle":"2023-11-13T22:19:31.403964Z","shell.execute_reply":"2023-11-13T22:19:31.402963Z","shell.execute_reply.started":"2023-11-13T22:19:30.254894Z"},"trusted":true},"outputs":[],"source":["file_path = 'augmented_data.npz'\n","loaded_data = np.load(file_path, allow_pickle = True)\n","\n","# Access the 'data' and 'labels' arrays\n","data = loaded_data['data']\n","labels = loaded_data['labels']\n","\n","print(len(data))\n","print(len(labels))\n","\n","healthy = []\n","unhealthy = []\n","\n","# Iterate through the images to separate them into the proper set\n","for i, image in enumerate(data):\n","    if labels[i] == 'unhealthy':\n","        unhealthy.append(image)\n","    else:\n","        healthy.append(image)\n","\n","print(len(healthy))\n","print(len(unhealthy))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:19:31.405704Z","iopub.status.busy":"2023-11-13T22:19:31.405438Z","iopub.status.idle":"2023-11-13T22:19:32.435995Z","shell.execute_reply":"2023-11-13T22:19:32.432404Z","shell.execute_reply.started":"2023-11-13T22:19:31.405679Z"},"trusted":true},"outputs":[],"source":["# Concatenate 'healthy' and 'unhealthy' arrays along axis 0\n","X = np.concatenate([healthy, unhealthy], axis = 0)\n","\n","# Create labels: 1 for 'healthy', 0 for 'unhealthy'\n","y = np.concatenate([np.ones(len(healthy)), np.zeros(len(unhealthy))], axis = 0)\n","\n","# y = tfk.utils.to_categorical(y,len(np.unique(y)))\n","y = np.expand_dims(y, axis=-1)  # Add a singleton dimension for compatibility with binary classification\n","\n","# Split data into train and val\n","X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.2, stratify=np.argmax(y,axis=1))\n","\n","# Print shapes of the datasets\n","print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n","print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:19:32.437623Z","iopub.status.busy":"2023-11-13T22:19:32.437242Z","iopub.status.idle":"2023-11-13T22:19:32.443781Z","shell.execute_reply":"2023-11-13T22:19:32.442765Z","shell.execute_reply.started":"2023-11-13T22:19:32.437592Z"},"trusted":true},"outputs":[],"source":["# Define input shape, output shape, batch size, and number of epochs\n","input_shape = X_train.shape[1:]\n","output_shape = y_train.shape[1:]\n","batch_size = 32\n","epochs = 100\n","\n","img_height = 96\n","img_width = 96\n","\n","# Print input shape, batch size, and number of epochs\n","print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:19:32.446152Z","iopub.status.busy":"2023-11-13T22:19:32.445798Z","iopub.status.idle":"2023-11-13T22:19:32.453975Z","shell.execute_reply":"2023-11-13T22:19:32.453170Z","shell.execute_reply.started":"2023-11-13T22:19:32.446126Z"},"trusted":true},"outputs":[],"source":["callbacks = [\n","    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, mode='auto'),\n","]"]},{"cell_type":"markdown","metadata":{},"source":["### Train the (sequential) model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Macs with Apple Silicon suffer performance penalties when using the modern Adam optimizer\n","# Detect system specs and select the appropriate optimizer\n","\n","if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n","    optimizer = tf.keras.optimizers.legacy.Adam()\n","else:\n","    optimizer = tf.keras.optimizers.Adam()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:19:32.456736Z","iopub.status.busy":"2023-11-13T22:19:32.456429Z","iopub.status.idle":"2023-11-13T22:19:32.471387Z","shell.execute_reply":"2023-11-13T22:19:32.470577Z","shell.execute_reply.started":"2023-11-13T22:19:32.456711Z"},"trusted":true},"outputs":[],"source":["def build_model(input_shape=input_shape, dropout_rate=0.4):\n","    tf.random.set_seed(seed)\n","\n","    input_layer = tf.keras.Input(shape=input_shape, name='Input')\n","\n","    conv1 = tf.keras.layers.Conv2D(\n","        filters=32,\n","        kernel_size=3,\n","        padding='same',\n","        activation='relu',\n","        kernel_initializer=tf.keras.initializers.HeUniform(seed)\n","    )(input_layer)\n","    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n","    pool1 = tf.keras.layers.MaxPooling2D()(conv1)\n","    pool1 = tf.keras.layers.Dropout(dropout_rate, seed=seed)(pool1)\n","\n","    conv2 = tf.keras.layers.Conv2D(\n","        filters=64,\n","        kernel_size=3,\n","        padding='same',\n","        activation='relu',\n","        kernel_initializer=tf.keras.initializers.HeUniform(seed)\n","    )(pool1)\n","    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n","    pool2 = tf.keras.layers.MaxPooling2D()(conv2)\n","    pool2 = tf.keras.layers.Dropout(dropout_rate, seed=seed)(pool2)\n","\n","    conv3 = tf.keras.layers.Conv2D(\n","        filters=128,\n","        kernel_size=3,\n","        padding='same',\n","        activation='relu',\n","        kernel_initializer=tf.keras.initializers.HeUniform(seed)\n","    )(pool2)\n","    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n","    pool3 = tf.keras.layers.MaxPooling2D()(conv3)\n","    pool3 = tf.keras.layers.Dropout(dropout_rate, seed=seed)(pool3)\n","    \n","    final_pool = pool3\n","\n","    gap_layer = tf.keras.layers.GlobalAveragePooling2D(name='Gap')(final_pool)\n","    \n","    dropout = tf.keras.layers.Dropout(dropout_rate, seed=seed)(gap_layer)\n","\n","    l1_strength = 0.0005\n","    l2_strength = 0.01\n","    \n","    # Use 1 unit for binary classification (2 classes)\n","    output_layer = tf.keras.layers.Dense(\n","        units=1,\n","        activation='sigmoid',  # Use 'sigmoid' for binary classification\n","        kernel_initializer=tf.keras.initializers.HeUniform(seed),\n","        name='output_layer',\n","        kernel_regularizer = tf.keras.regularizers.L1L2(l1=l1_strength, l2=l2_strength)\n","    )(dropout)\n","\n","    # Define the learning rate schedule\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","        initial_learning_rate=0.0005,\n","        decay_steps=10000,\n","        decay_rate=0.9\n","    )\n","\n","    # Connect input and output through the Model class\n","    model = tf.keras.Model(\n","        inputs=input_layer,\n","        outputs=output_layer,\n","        name='model')\n","\n","    # Compile the model with the customized optimizer\n","    model.compile(\n","        loss=tf.keras.losses.BinaryCrossentropy(),\n","        optimizer=optimizer,\n","        metrics=['accuracy'])\n","\n","    # Return the model\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:19:32.472607Z","iopub.status.busy":"2023-11-13T22:19:32.472342Z","iopub.status.idle":"2023-11-13T22:19:32.851005Z","shell.execute_reply":"2023-11-13T22:19:32.850036Z","shell.execute_reply.started":"2023-11-13T22:19:32.472583Z"},"trusted":true},"outputs":[],"source":["# Build the model and return a summary about it\n","model = build_model()\n","model.summary()\n","tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:19:32.852481Z","iopub.status.busy":"2023-11-13T22:19:32.852170Z"},"trusted":true},"outputs":[],"source":["# Train the model\n","history = model.fit(\n","    x = X_train,\n","    y = y_train,\n","    batch_size = batch_size,\n","    epochs = epochs,\n","    validation_data = (X_val, y_val),\n","    callbacks = callbacks\n",").history\n"]},{"cell_type":"markdown","metadata":{},"source":["### Validate the (sequential) model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["current_datetime = datetime.now()\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Create the zipfile with date and time in the name\n","zipfile_name = f\"model_{formatted_datetime}.zip\"\n","model.save(zipfile_name)\n","\n","X_test_1 = X_val\n","X_test_2 = rotation(X_val)\n","X_test_3 = flip(X_val)\n","\n","true_labels = y_val\n","\n","# Make predictions on each test set\n","predictions_1 = model.predict(X_test_1)\n","predictions_2 = model.predict(X_test_2)\n","predictions_3 = model.predict(X_test_3)\n","\n","# Assuming binary classification, adjust if needed\n","\n","best_accuracy_1 = 0.0\n","limit_1 = 0.0\n","\n","for limit in range(100):\n","    predicted_labels_1 = (predictions_1 > (limit / 100.0)).astype(int)\n","    accuracy_1 = np.sum(predicted_labels_1 == true_labels) / len(true_labels)\n","    \n","    if (accuracy_1 < best_accuracy_1):\n","        continue\n","        \n","    best_accuracy_1 = accuracy_1\n","    limit_1 = (limit / 100.0)\n","        \n","best_accuracy_2 = 0.0\n","limit_2 = 0.0\n","\n","for limit in range(100):\n","    predicted_labels_2 = (predictions_2 > (limit / 100.0)).astype(int)\n","    accuracy_2 = np.sum(predicted_labels_2 == true_labels) / len(true_labels)\n","    \n","    if (accuracy_2 < best_accuracy_2):\n","        continue\n","        \n","    best_accuracy_2 = accuracy_2\n","    limit_2 = (limit / 100.0)\n","\n","best_accuracy_3 = 0.0\n","limit_3 = 0.0\n","\n","for limit in range(100):\n","    predicted_labels_3 = (predictions_3 > (limit / 100.0)).astype(int)\n","    accuracy_3 = np.sum(predicted_labels_3 == true_labels) / len(true_labels)\n","    \n","    if (accuracy_3 < best_accuracy_3):\n","        continue\n","        \n","    best_accuracy_3 = accuracy_3\n","    limit_3 = (limit / 100.0)\n","\n","\n","print(\"Limit on Test Set 1:\", limit_1)\n","print(\"Limit on Test Set 2:\", limit_2)\n","print(\"Limit on Test Set 3:\", limit_3)\n","        \n","predicted_labels_1 = (predictions_1 > limit_1).astype(int)\n","predicted_labels_2 = (predictions_2 > limit_2).astype(int)\n","predicted_labels_3 = (predictions_3 > limit_3).astype(int)\n","\n","# Compare the results\n","accuracy_1 = np.sum(predicted_labels_1 == true_labels) / len(true_labels)\n","accuracy_2 = np.sum(predicted_labels_2 == true_labels) / len(true_labels)\n","accuracy_3 = np.sum(predicted_labels_3 == true_labels) / len(true_labels)\n","\n","print(\"Accuracy on Test Set 1:\", accuracy_1)\n","print(\"Accuracy on Test Set 2:\", accuracy_2)\n","print(\"Accuracy on Test Set 3:\", accuracy_3)\n","\n","\n","final_prediction = []\n","\n","for i in range(len(predictions_1)):\n","    healthy_prediction = predicted_labels_1[i] + predicted_labels_2[i] + predicted_labels_3[i]\n","    \n","    if (healthy_prediction >= 2):\n","        final_prediction.append([1])\n","        \n","    else:\n","        final_prediction.append([0])\n","        \n","final_accuracy = np.sum(final_prediction == true_labels) / len(true_labels)\n","\n","print(\"Accuracy of Final Prediction:\", final_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# N predictions to display\n","N = 10\n","\n","# Output the first N predictions for each set\n","print(\"Predictions on Test Set 1:\")\n","print(predictions_1[:N])\n","\n","print(\"\\nPredictions on Test Set 2:\")\n","print(predictions_2[:N])\n","\n","print(\"\\nPredictions on Test Set 3:\")\n","print(predictions_3[:N])\n","\n","print(\"\\nFinal Predictions:\")\n","print(np.array(final_prediction[:N]))\n","\n","# Create subplots\n","fig, axes = plt.subplots(1, N, figsize=(15, 5))\n","\n","for i in range(N):\n","    # Plot the original image from X_val\n","    ax = axes[i]\n","    ax.imshow(X_val[i].astype(np.uint8))\n","    ax.set_title(y_val[i])\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plot the training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the training\n","plt.figure(figsize=(15, 5))\n","plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n","plt.plot(history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n","plt.legend(loc='upper left')\n","plt.title('Accuracy')\n","plt.grid(alpha=.3)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# K fold"]},{"cell_type":"markdown","metadata":{},"source":["### train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T20:05:48.377108Z","iopub.status.busy":"2023-11-13T20:05:48.376289Z","iopub.status.idle":"2023-11-13T20:05:48.772491Z","shell.execute_reply":"2023-11-13T20:05:48.771066Z","shell.execute_reply.started":"2023-11-13T20:05:48.377074Z"},"trusted":true},"outputs":[],"source":["# Train the model\n","# Define the number of folds\n","num_folds = 5\n","kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","# Initialize lists to store the results from each fold\n","all_train_loss = []\n","all_train_acc = []\n","all_val_loss = []\n","all_val_acc = []\n","\n","initial_learning_rate = 0.0001\n","lr_schedule = tfk.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate,\n","    decay_steps = 10000,\n","    decay_rate = 0.9,\n","    staircase = True,\n",")\n","\n","# Iterate over the folds\n","for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n","    print(f\"Fold {fold + 1}/{num_folds}\")\n","\n","    # Split the data into training and validation sets\n","    X_train, X_val = X[train_index], X[val_index]\n","    y_train, y_val = y[train_index], y[val_index]\n","\n","    # Build the model\n","    model = build_model(input_shape=X_train.shape[1:], dropout_rate=0.4)\n","\n","    # Train the model\n","    history = model.fit(\n","        X_train, y_train,\n","        epochs = epochs,\n","        batch_size = batch_size,\n","        validation_data = (X_val, y_val),\n","        verbose = 1\n","    )\n","\n","    # Evaluate on training set\n","    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n","    all_train_loss.append(train_loss)\n","    all_train_acc.append(train_acc)\n","\n","    # Evaluate on validation set\n","    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n","    all_val_loss.append(val_loss)\n","    all_val_acc.append(val_acc)\n","\n","# Print average results over all folds\n","print(f\"Average Training Loss: {np.mean(all_train_loss)}\")\n","print(f\"Average Training Accuracy: {np.mean(all_train_acc)}\")\n","print(f\"Average Validation Loss: {np.mean(all_val_loss)}\")\n","print(f\"Average Validation Accuracy: {np.mean(all_val_acc)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Plot the training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the training\n","plt.figure(figsize=(15, 5))\n","plt.plot(history.history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n","plt.plot(history.history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n","plt.legend(loc='upper left')\n","plt.title('Accuracy')\n","plt.grid(alpha=.3)\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Save the model and create submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import zipfile\n","from datetime import datetime\n","import tensorflow as tf\n","from os.path import basename\n","\n","# Specify the name of the submission folder\n","submission_folder = \"SubmissionModel\"\n","\n","# Save best epoch model\n","model.save(os.path.join(submission_folder, \"SubmissionModel\"))\n","\n","# Save the model.py file in the main directory\n","with open(\"model.py\", \"w\") as model_file:\n","    model_file.write(\"\"\"\n","\n","import os\n","import tensorflow as tf\n","from tensorflow.keras import layers as tfkl\n","\n","class model:\n","    def __init__(self, path):\n","        self.model = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel'))\n","\n","    def predict(self, X):\n","        # Define the RandomRotation layer\n","        rotation = tf.keras.Sequential([\n","            tfkl.RandomRotation(0.3),\n","        ])\n","\n","        # Define the RandomFlip layer\n","        flip = tf.keras.Sequential([\n","            tfkl.RandomFlip(\"vertical\"),\n","        ])\n","\n","        limit = tf.constant(0.56, dtype=tf.float32)\n","\n","        X_test_1 = X\n","        X_test_2 = rotation(X) \n","        X_test_3 = flip(X)\n","\n","        out1 = self.model.predict(X_test_1)\n","        out2 = self.model.predict(X_test_2)\n","        out3 = self.model.predict(X_test_3)\n","\n","        out1 = tf.argmax(out1, axis=-1)  # Shape [BS]\n","        out2 = tf.argmax(out2, axis=-1)  # Shape [BS]\n","        out3 = tf.argmax(out3, axis=-1)  # Shape [BS]\n","        \n","        out = []  # Initialize out as an empty list\n","        \n","        for i in range(len(out1)):\n","            hp = 0\n","        \n","            if tf.cast(out1[i], tf.float32) > limit:\n","                hp += 1\n","        \n","            if tf.cast(out2[i], tf.float32) > limit:\n","                hp += 1\n","        \n","            if tf.cast(out3[i], tf.float32) > limit:\n","                hp += 1\n","        \n","            if hp >= 2:\n","                out.append([1])\n","            else:\n","                out.append([0])\n","\n","        out = tf.argmax(out, axis=-1)  # Shape [BS]\n","\n","        return out\n","\"\"\")\n","\n","# Create an empty metadata file in the main directory\n","open(\"metadata\", \"w\").close()\n","\n","# Get the current date and time\n","current_datetime = datetime.now()\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Create the zipfile with date and time in the name\n","zipfile_name = f\"{submission_folder}_{formatted_datetime}.zip\"\n","\n","with zipfile.ZipFile(zipfile_name, 'w') as zip_file:\n","    # Add the entire \"SubmissionModel\" folder and its contents to the archive\n","    for foldername, subfolders, filenames in os.walk(submission_folder):\n","        for filename in filenames:\n","            file_path = os.path.join(foldername, filename)\n","            arcname = os.path.relpath(file_path, submission_folder)\n","            \n","            if (foldername in [\"SubmissionModel\", \"SubmissionModel/model\", \"SubmissionModel/model/variables\", \"SubmissionModel/variables\"]):\n","                continue\n","        \n","            # print(foldername)\n","            print(file_path)\n","            \n","            zip_file.write(file_path, arcname)\n","\n","    # Add other files to the archive (model.py and metadata)\n","    zip_file.write(\"model.py\", arcname=\"model.py\")\n","    zip_file.write(\"metadata\", arcname=\"metadata\")\n","\n","print(zipfile_name)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
