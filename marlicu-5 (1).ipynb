{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6967079,"sourceType":"datasetVersion","datasetId":4002762}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# Fix randomness and hide warnings\nseed = 42\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\nimport numpy as np\nnp.random.seed(seed)\n\nimport logging\n\nimport random\nrandom.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T21:59:37.451932Z","iopub.execute_input":"2023-11-17T21:59:37.452803Z","iopub.status.idle":"2023-11-17T21:59:37.466169Z","shell.execute_reply.started":"2023-11-17T21:59:37.452765Z","shell.execute_reply":"2023-11-17T21:59:37.465061Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T21:59:37.489485Z","iopub.execute_input":"2023-11-17T21:59:37.489863Z","iopub.status.idle":"2023-11-17T21:59:50.434202Z","shell.execute_reply.started":"2023-11-17T21:59:37.489835Z","shell.execute_reply":"2023-11-17T21:59:50.433039Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.13.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install imagehash\n!pip install seaborn","metadata":{"execution":{"iopub.status.busy":"2023-11-17T21:59:50.436006Z","iopub.execute_input":"2023-11-17T21:59:50.436601Z","iopub.status.idle":"2023-11-17T22:00:18.905729Z","shell.execute_reply.started":"2023-11-17T21:59:50.436559Z","shell.execute_reply":"2023-11-17T22:00:18.904626Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: imagehash in /opt/conda/lib/python3.10/site-packages (4.3.1)\nRequirement already satisfied: PyWavelets in /opt/conda/lib/python3.10/site-packages (from imagehash) (1.4.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from imagehash) (1.24.3)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from imagehash) (10.1.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from imagehash) (1.11.3)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.24.3)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.0.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.7.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import other libraries\nimport cv2\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nimport seaborn as sns\nfrom PIL import Image\nimport imagehash\nfrom sklearn.model_selection import KFold\nimport zipfile","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:02:29.573364Z","iopub.execute_input":"2023-11-17T22:02:29.574208Z","iopub.status.idle":"2023-11-17T22:02:30.583648Z","shell.execute_reply.started":"2023-11-17T22:02:29.574172Z","shell.execute_reply":"2023-11-17T22:02:30.582724Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load and clean the dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Load the data from the file\nfile_path = \"/kaggle/input/public-data-zip/public_data.npz\"\nloaded_data = np.load(file_path, allow_pickle = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:00:26.702289Z","iopub.execute_input":"2023-11-17T22:00:26.702713Z","iopub.status.idle":"2023-11-17T22:00:26.726294Z","shell.execute_reply.started":"2023-11-17T22:00:26.702674Z","shell.execute_reply":"2023-11-17T22:00:26.725209Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Access the 'data' and 'labels' arrays\ndata = loaded_data['data']\nlabels = loaded_data['labels']","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:00:26.860192Z","iopub.execute_input":"2023-11-17T22:00:26.860950Z","iopub.status.idle":"2023-11-17T22:00:31.507002Z","shell.execute_reply.started":"2023-11-17T22:00:26.860916Z","shell.execute_reply":"2023-11-17T22:00:31.506131Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 58 -> Shrek - 338 -> Trololo\nto_extract = [data[58], data[338]]\noutliers = []\n\nfor i, image in enumerate(to_extract):\n    pil_image = Image.fromarray(image.astype('uint8'))\n    img_hash = imagehash.average_hash(pil_image)\n\n    outliers.append(img_hash)\n    print(img_hash)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:02:56.474396Z","iopub.execute_input":"2023-11-17T22:02:56.474808Z","iopub.status.idle":"2023-11-17T22:02:56.488265Z","shell.execute_reply.started":"2023-11-17T22:02:56.474775Z","shell.execute_reply":"2023-11-17T22:02:56.487262Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"081c1e1e1e3f3f1b\n02393d3e3c3c1c00\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a list to store unique hashes and their corresponding indices\nhashes = []\nunique_indices = []\nraw_images = []\nduplicates = []\nduplicates_labels = []  # Assuming you have a list of labels for duplicate images\nindices_duplicates = []\n\n# Iterate through the images and calculate their perceptual hashes\nfor i, image in enumerate(data):\n    pil_image = Image.fromarray(image.astype('uint8'))\n    img_hash = imagehash.average_hash(pil_image)\n\n    pil_image.info['label'] = labels[i]\n    raw_images.append(pil_image)\n\n    # Check if the hash is unique\n    if img_hash not in hashes:\n        hashes.append(img_hash)\n        unique_indices.append(i)\n    else:\n        duplicates.append(np.array(pil_image))  # Convert PIL Image to NumPy array\n        duplicates_labels.append(labels[i])\n        indices_duplicates.append(i)\n\n# Create a new numpy array with only the unique images\nunique_data = data[unique_indices]\n\n# Create a new numpy array with only the unique labels\nunique_labels = labels[unique_indices]\n\n# Save the cleaned dataset\nnp.savez('cleaned_data.npz', data=unique_data, labels=unique_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:02:57.582467Z","iopub.execute_input":"2023-11-17T22:02:57.583244Z","iopub.status.idle":"2023-11-17T22:05:01.557283Z","shell.execute_reply.started":"2023-11-17T22:02:57.583207Z","shell.execute_reply":"2023-11-17T22:05:01.556244Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Image augmentation","metadata":{}},{"cell_type":"code","source":"# Load the data from the file\nfile_path = \"/kaggle/working/cleaned_data.npz\"\nloaded_data = np.load(file_path, allow_pickle = True)\n\n# Access the 'data' and 'labels' arrays\ndata = loaded_data['data']\nlabels = loaded_data['labels']\n\nhealthy = []\nunhealthy = []\n\n# Iterate through the images to separate them into the proper set\nfor i, image in enumerate(data):\n    if labels[i] == 'unhealthy':\n        unhealthy.append(image)\n    else:\n        healthy.append(image)\n\nprint(len(healthy))\nprint(len(unhealthy))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:05:01.559037Z","iopub.execute_input":"2023-11-17T22:05:01.559333Z","iopub.status.idle":"2023-11-17T22:05:02.208739Z","shell.execute_reply.started":"2023-11-17T22:05:01.559307Z","shell.execute_reply":"2023-11-17T22:05:02.207700Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"3061\n1791\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\n\ndataset = healthy\n\ndef apply_rotations(image):\n    angles = np.arange(0, 359,95)\n    rotated_images = []\n    for angle in angles:\n        image = (image * 255).astype(np.uint8)\n        \n        rotated_image = Image.fromarray(image)\n        rotated_image = rotated_image.rotate(angle)\n        rotated_images.append(np.array(rotated_image))\n    return rotated_images\n\n# Apply rotations to the dataset\nrotated_healthy = []\nfor image in dataset:\n    rotated_images = apply_rotations(image)\n    rotated_healthy.extend(rotated_images)\n\nrotated_healthy = np.stack(rotated_healthy)\n\nprint(len(healthy))\nprint(len(rotated_healthy))\n\npreprocessed_healthy = rotated_healthy\n\nprint(len(preprocessed_healthy))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:05:02.210104Z","iopub.execute_input":"2023-11-17T22:05:02.210480Z","iopub.status.idle":"2023-11-17T22:05:04.698128Z","shell.execute_reply.started":"2023-11-17T22:05:02.210450Z","shell.execute_reply":"2023-11-17T22:05:04.697100Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"3061\n12244\n12244\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\n\ndataset = unhealthy\n\ndef apply_rotations(image):\n    angles = np.arange(0, 359, 55)\n    rotated_images = []\n    for angle in angles:\n        image = (image * 255).astype(np.uint8)\n        \n        rotated_image = Image.fromarray(image)\n        rotated_image = rotated_image.rotate(angle)\n        rotated_images.append(np.array(rotated_image))\n    return rotated_images\n\n# Apply rotations to the dataset\nrotated_unhealthy = []\nfor image in dataset:\n    rotated_images = apply_rotations(image)\n    rotated_unhealthy.extend(rotated_images)\n\nrotated_unhealthy = np.stack(rotated_unhealthy)\n\nprint(len(unhealthy))\nprint(len(rotated_unhealthy))\n\npreprocessed_unhealthy = rotated_unhealthy\n\nprint(len(preprocessed_unhealthy))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:05:04.700321Z","iopub.execute_input":"2023-11-17T22:05:04.700647Z","iopub.status.idle":"2023-11-17T22:05:06.885479Z","shell.execute_reply.started":"2023-11-17T22:05:04.700616Z","shell.execute_reply":"2023-11-17T22:05:06.884407Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1791\n12537\n12537\n","output_type":"stream"}]},{"cell_type":"code","source":"# Concatenate the preprocessed sets\npreprocessed_data = np.concatenate([preprocessed_healthy, preprocessed_unhealthy], axis=0)\naugmented_data = preprocessed_data\n\naugmented_labels_healthy = np.full((len(preprocessed_healthy),), 'healthy', dtype='object')\naugmented_labels_unhealthy = np.full((len(preprocessed_unhealthy),), 'unhealthy', dtype='object')\n\naugmented_labels = np.concatenate([augmented_labels_healthy, augmented_labels_unhealthy], axis=0)\n\nnp.savez('augmented_data.npz', data=augmented_data, labels=augmented_labels)\n\nprint(len(augmented_data))\nprint(len(augmented_labels))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:05:06.886728Z","iopub.execute_input":"2023-11-17T22:05:06.887050Z","iopub.status.idle":"2023-11-17T22:05:08.325799Z","shell.execute_reply.started":"2023-11-17T22:05:06.887022Z","shell.execute_reply":"2023-11-17T22:05:08.324685Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"24781\n24781\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load and process the (augmented) dataset","metadata":{}},{"cell_type":"code","source":"file_path = 'augmented_data.npz'\nloaded_data = np.load(file_path, allow_pickle = True)\n\n# Access the 'data' and 'labels' arrays\nX = loaded_data['data']\nlabels = loaded_data['labels']\n\nprint(len(X))\nprint(len(labels))\n\nX = X / 255.0\n\nhealthy = []\nunhealthy = []\n\n# Iterate through the images to separate them into the proper set\nfor i, image in enumerate(X):\n    if labels[i] == 'unhealthy':\n        unhealthy.append(image)\n    else:\n        healthy.append(image)\n\nnp.random.shuffle(healthy)\nnp.random.shuffle(unhealthy)\n\nprint(len(healthy))\nprint(len(unhealthy))\n\nlabel_dict = {'healthy': 0, 'unhealthy': 1}\ny = np.array([label_dict[label] for label in labels]).reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:10:25.825262Z","iopub.execute_input":"2023-11-17T22:10:25.825705Z","iopub.status.idle":"2023-11-17T22:10:28.657550Z","shell.execute_reply.started":"2023-11-17T22:10:25.825652Z","shell.execute_reply":"2023-11-17T22:10:28.656447Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"24781\n24781\n12244\n12537\n","output_type":"stream"}]},{"cell_type":"code","source":"# Codifica delle etichette 'healthy' e 'unhealthy' in numeri\nlabel_dict = {'healthy': 0, 'unhealthy': 1}\ny = np.array([label_dict[label] for label in labels]) # 0 è [1,0]\n\n# Convert labels to one-hot encoding format\ny = tfk.utils.to_categorical(y,2)\n\n# Split data into train and val\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.2, stratify=y)\n\n# Print shapes of the datasets\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:10:28.660044Z","iopub.execute_input":"2023-11-17T22:10:28.660491Z","iopub.status.idle":"2023-11-17T22:10:30.489603Z","shell.execute_reply.started":"2023-11-17T22:10:28.660450Z","shell.execute_reply":"2023-11-17T22:10:30.488611Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"X_train shape: (19824, 96, 96, 3), y_train shape: (19824, 2)\nX_val shape: (4957, 96, 96, 3), y_val shape: (4957, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\n\n# Reshape arrays if necessary (replace (96, 96, 3) with the actual shape of your images)\nX_train = X_train.reshape((len(X_train), -1))\nX_val = X_val.reshape((len(X_val), -1))\n\n# Apply SMOTE to the training set\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_train, y_train = smote.fit_resample(X_train, y_train)\n\n#Apply SMOTE to the training val\nX_val, y_val = smote.fit_resample(X_val, y_val)\n\n# Reshape arrays back to the original shape\nX_train = X_train.reshape((len(X_train), 96, 96, 3))\nX_val = X_val.reshape((len(X_val), 96, 96, 3))\n\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:10:30.490696Z","iopub.execute_input":"2023-11-17T22:10:30.490970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define input shape, output shape, batch size, and number of epochs\ninput_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 32\nepochs = 100\n\nimg_height = 96\nimg_width = 96\n\n# Print input shape, batch size, and number of epochs\nprint(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"markdown","source":"## ResNet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n\n# Assuming you have defined 'input_shape' somewhere in your code\ninput_shape = (96, 96, 3)\n\n# Load pre-trained ResNet50 model without the top (classification) layer\nbase_model_tf = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Model building\nbase_model_tf.trainable = False\n\npt = Input(shape=input_shape)\nfunc = tf.cast(pt, tf.float32)\nmodel_resnet = base_model_tf(func, training=False)\nmodel_resnet = GlobalAveragePooling2D()(model_resnet)\nmodel_resnet = Dense(128, activation='relu')(model_resnet)\nmodel_resnet = Dense(64, activation='relu')(model_resnet)\nmodel_resnet = Dense(1, activation='sigmoid')(model_resnet)  # Change to 1 neuron and 'sigmoid' activation\n\nmodel_main = Model(inputs=pt, outputs=model_resnet)\nmodel_main.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:08:04.722509Z","iopub.execute_input":"2023-11-17T22:08:04.722960Z","iopub.status.idle":"2023-11-17T22:08:12.404565Z","shell.execute_reply.started":"2023-11-17T22:08:04.722923Z","shell.execute_reply":"2023-11-17T22:08:12.403523Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94765736/94765736 [==============================] - 1s 0us/step\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n tf.cast (TFOpLambda)        (None, 96, 96, 3)         0         \n                                                                 \n resnet50 (Functional)       (None, 3, 3, 2048)        23587712  \n                                                                 \n global_average_pooling2d (  (None, 2048)              0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dense (Dense)               (None, 128)               262272    \n                                                                 \n dense_1 (Dense)             (None, 64)                8256      \n                                                                 \n dense_2 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 23858305 (91.01 MB)\nTrainable params: 270593 (1.03 MB)\nNon-trainable params: 23587712 (89.98 MB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Callbacks\nes = EarlyStopping(monitor='val_accuracy', verbose=1, patience=7, mode='auto')\nmc = ModelCheckpoint(filepath='/kaggle/working/best_model/', monitor='val_accuracy', verbose=1, save_best_only=True)\nlr = ReduceLROnPlateau(monitor='val_accuracy', verbose=1, patience=5, min_lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:08:26.204479Z","iopub.execute_input":"2023-11-17T22:08:26.204838Z","iopub.status.idle":"2023-11-17T22:08:26.214202Z","shell.execute_reply.started":"2023-11-17T22:08:26.204812Z","shell.execute_reply":"2023-11-17T22:08:26.213150Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\nmodel_main.compile(optimizer=Adam(), loss=BinaryCrossentropy(), metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:08:26.401857Z","iopub.execute_input":"2023-11-17T22:08:26.402254Z","iopub.status.idle":"2023-11-17T22:08:26.476439Z","shell.execute_reply.started":"2023-11-17T22:08:26.402223Z","shell.execute_reply":"2023-11-17T22:08:26.475333Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n# One-hot encode the target labels\ny_train_one_hot = to_categorical(y_train, num_classes=2)\ny_val_one_hot = to_categorical(y_val, num_classes=2)\n\n#Training\nhistory = model_main.fit(\n    x=X_train,\n    y=y_train_one_hot,\n    validation_data=(X_val, y_val_one_hot),\n    epochs=30,\n    steps_per_epoch=200,\n    verbose=1,\n    callbacks=[mc, es, lr]\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T22:08:26.689392Z","iopub.execute_input":"2023-11-17T22:08:26.689784Z","iopub.status.idle":"2023-11-17T22:08:34.589763Z","shell.execute_reply.started":"2023-11-17T22:08:26.689751Z","shell.execute_reply":"2023-11-17T22:08:34.588313Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m y_val_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(y_val, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Training\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_main\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_one_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_one_hot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_filejgnt9frk.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2432, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5809, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 2)).\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2432, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5809, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 2)).\n","output_type":"error"}]},{"cell_type":"code","source":"# Optionally, you can visualize the training history\nimport matplotlib.pyplot as plt\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nmodel = load_model('/kaggle/working/best_model')\n\npredictions = model.predict(X)\npredicted_class = np.argmax(predictions[i])\n\nfor i in range(10):\n    print(predictions[i], labels[i])\n\npredicted_labels = (predicted_class > 0.5).astype(int)\naccuracy = np.sum(predictions == (labels == \"healthy\")) / len(labels)\n\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN","metadata":{}},{"cell_type":"code","source":"def build_model(input_shape=input_shape, dropout_rate=0.4):\n    tf.random.set_seed(seed)\n\n    input_layer = tf.keras.Input(shape=input_shape, name='Input')\n\n    conv1 = tf.keras.layers.Conv2D(\n        filters=32,\n        kernel_size=3,\n        padding='same',\n        activation='relu',\n        kernel_initializer=tf.keras.initializers.HeUniform(seed)\n    )(input_layer)\n    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n    pool1 = tf.keras.layers.MaxPooling2D()(conv1)\n    pool1 = tf.keras.layers.Dropout(dropout_rate, seed=seed)(pool1)\n\n    conv2 = tf.keras.layers.Conv2D(\n        filters=64,\n        kernel_size=3,\n        padding='same',\n        activation='relu',\n        kernel_initializer=tf.keras.initializers.HeUniform(seed)\n    )(pool1)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n    pool2 = tf.keras.layers.MaxPooling2D()(conv2)\n    pool2 = tf.keras.layers.Dropout(dropout_rate, seed=seed)(pool2)\n\n    conv3 = tf.keras.layers.Conv2D(\n        filters=128,\n        kernel_size=3,\n        padding='same',\n        activation='relu',\n        kernel_initializer=tf.keras.initializers.HeUniform(seed)\n    )(pool2)\n    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n    pool3 = tf.keras.layers.MaxPooling2D()(conv3)\n    pool3 = tf.keras.layers.Dropout(dropout_rate, seed=seed)(pool3)\n\n#    conv4 = tf.keras.layers.Conv2D(\n#        filters=256,\n#        kernel_size=3,\n#        padding='same',\n#        activation='relu',\n#        kernel_initializer=tf.keras.initializers.HeUniform(seed)\n#    )(pool3)\n#    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n#    pool4 = tf.keras.layers.MaxPooling2D()(conv4)\n#    pool4 = tf.keras.layers.Dropout(dropout_rate, seed=seed)(pool4)\n    \n    final_pool = pool3\n\n    gap_layer = tf.keras.layers.GlobalAveragePooling2D(name='Gap')(final_pool)\n    \n    dropout = tf.keras.layers.Dropout(dropout_rate, seed=seed)(gap_layer)\n\n    l1_strength = 0.0005\n    l2_strength = 0.01\n    \n    # Use 1 unit for binary classification (2 classes)\n    output_layer = tf.keras.layers.Dense(\n        units=1,\n        activation='sigmoid',  # Use 'sigmoid' for binary classification\n        kernel_initializer=tf.keras.initializers.HeUniform(seed),\n        name='output_layer',\n        kernel_regularizer = tf.keras.regularizers.L1L2(l1=l1_strength, l2=l2_strength)\n    )(dropout)\n\n    # Define the learning rate schedule\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=0.0005,\n        decay_steps=10000,\n        decay_rate=0.9\n    )\n    \n    # Use the learning rate schedule for the Adam optimizer\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n    # Connect input and output through the Model class\n    model = tf.keras.Model(\n        inputs=input_layer,\n        outputs=output_layer,\n        name='model')\n\n    # Compile the model with the customized optimizer\n    model.compile(\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        optimizer=optimizer,\n        metrics=['accuracy'])\n\n    # Return the model\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:19:32.456429Z","iopub.execute_input":"2023-11-13T22:19:32.456736Z","iopub.status.idle":"2023-11-13T22:19:32.471387Z","shell.execute_reply.started":"2023-11-13T22:19:32.456711Z","shell.execute_reply":"2023-11-13T22:19:32.470577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the model and return a summary about it\nmodel = build_model()\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:19:32.472342Z","iopub.execute_input":"2023-11-13T22:19:32.472607Z","iopub.status.idle":"2023-11-13T22:19:32.851005Z","shell.execute_reply.started":"2023-11-13T22:19:32.472583Z","shell.execute_reply":"2023-11-13T22:19:32.850036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = (X_val, y_val),\n    callbacks = callbacks\n).history\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:19:32.852170Z","iopub.execute_input":"2023-11-13T22:19:32.852481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"# Define the RandomRotation layer\nrotation = tf.keras.Sequential([\n  tfkl.RandomRotation(0.3),\n])\n\n# Define the RandomFlip layer\nflip = tf.keras.Sequential([\n  tfkl.RandomFlip(\"vertical\"),\n])\n\nX_test_1 = X_val\nX_test_2 = rotation(X_val)\nX_test_3 = flip(X_val)\n\ntrue_labels = y_val\n\n# Make predictions on each test set\npredictions_1 = model.predict(X_test_1)\npredictions_2 = model.predict(X_test_2)\npredictions_3 = model.predict(X_test_3)\n\n# Assuming binary classification, adjust if needed\n\nbest_accuracy_1 = 0.0\nlimit_1 = 0.0\n\nfor limit in range(100):\n    predicted_labels_1 = (predictions_1 > (limit / 100.0)).astype(int)\n    accuracy_1 = np.sum(predicted_labels_1 == true_labels) / len(true_labels)\n    \n    if (accuracy_1 < best_accuracy_1):\n        continue\n        \n    best_accuracy_1 = accuracy_1\n    limit_1 = (limit / 100.0)\n        \nbest_accuracy_2 = 0.0\nlimit_2 = 0.0\n\nfor limit in range(100):\n    predicted_labels_2 = (predictions_2 > (limit / 100.0)).astype(int)\n    accuracy_2 = np.sum(predicted_labels_2 == true_labels) / len(true_labels)\n    \n    if (accuracy_2 < best_accuracy_2):\n        continue\n        \n    best_accuracy_2 = accuracy_2\n    limit_2 = (limit / 100.0)\n\nbest_accuracy_3 = 0.0\nlimit_3 = 0.0\n\nfor limit in range(100):\n    predicted_labels_3 = (predictions_3 > (limit / 100.0)).astype(int)\n    accuracy_3 = np.sum(predicted_labels_3 == true_labels) / len(true_labels)\n    \n    if (accuracy_3 < best_accuracy_3):\n        continue\n        \n    best_accuracy_3 = accuracy_3\n    limit_3 = (limit / 100.0)\n\n\nprint(\"Limit on Test Set 1:\", limit_1)\nprint(\"Limit on Test Set 2:\", limit_2)\nprint(\"Limit on Test Set 3:\", limit_3)\n        \npredicted_labels_1 = (predictions_1 > limit_1).astype(int)\npredicted_labels_2 = (predictions_2 > limit_2).astype(int)\npredicted_labels_3 = (predictions_3 > limit_3).astype(int)\n\n# Compare the results\naccuracy_1 = np.sum(predicted_labels_1 == true_labels) / len(true_labels)\naccuracy_2 = np.sum(predicted_labels_2 == true_labels) / len(true_labels)\naccuracy_3 = np.sum(predicted_labels_3 == true_labels) / len(true_labels)\n\nprint(\"Accuracy on Test Set 1:\", accuracy_1)\nprint(\"Accuracy on Test Set 2:\", accuracy_2)\nprint(\"Accuracy on Test Set 3:\", accuracy_3)\n\n\nfinal_prediction = []\n\nfor i in range(len(predictions_1)):\n    healthy_prediction = predicted_labels_1[i] + predicted_labels_2[i] + predicted_labels_3[i]\n    \n    if (healthy_prediction >= 2):\n        final_prediction.append([1])\n        \n    else:\n        final_prediction.append([0])\n        \nfinal_accuracy = np.sum(final_prediction == true_labels) / len(true_labels)\n\nprint(\"Accuracy of Final Prediction:\", final_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# N predictions to display\nN = 10\n\n# Output the first N predictions for each set\nprint(\"Predictions on Test Set 1:\")\nprint(predictions_1[:N])\n\nprint(\"\\nPredictions on Test Set 2:\")\nprint(predictions_2[:N])\n\nprint(\"\\nPredictions on Test Set 3:\")\nprint(predictions_3[:N])\n\nprint(\"\\nFinal Predictions:\")\nprint(np.array(final_prediction[:N]))\n\n# Create subplots\nfig, axes = plt.subplots(1, N, figsize=(15, 5))\n\nfor i in range(N):\n    # Plot the original image from X_val\n    ax = axes[i]\n    ax.imshow(X_val[i].astype(np.uint8))\n    ax.set_title(y_val[i])\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the training","metadata":{}},{"cell_type":"code","source":"# Plot the training\nplt.figure(figsize=(15, 5))\nplt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\nplt.plot(history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\nplt.legend(loc='upper left')\nplt.title('Accuracy')\nplt.grid(alpha=.3)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K fold","metadata":{}},{"cell_type":"code","source":"# Train the model\n# Define the number of folds\nnum_folds = 5\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n\n# Initialize lists to store the results from each fold\nall_train_loss = []\nall_train_acc = []\nall_val_loss = []\nall_val_acc = []\n\ninitial_learning_rate = 0.0001\nlr_schedule = tfk.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps = 10000,\n    decay_rate = 0.9,\n    staircase = True,\n)\n\n# Iterate over the folds\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n    print(f\"Fold {fold + 1}/{num_folds}\")\n\n    # Split the data into training and validation sets\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    # Build the model\n    model = build_model(input_shape=X_train.shape[1:], output_shape=output_shape)\n\n    # Compile the model with a custom learning rate\n    model.compile(\n        loss=tfk.losses.CategoricalCrossentropy(),\n        optimizer=tfk.optimizers.Adam(learning_rate=lr_schedule),\n        metrics=['accuracy']\n    )\n\n    # Train the model\n    history = model.fit(\n        X_train, y_train,\n        epochs = epochs,\n        batch_size = batch_size,\n        validation_data = (X_val, y_val),\n        verbose = 1\n    )\n\n    # Evaluate on training set\n    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n    all_train_loss.append(train_loss)\n    all_train_acc.append(train_acc)\n\n    # Evaluate on validation set\n    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n    all_val_loss.append(val_loss)\n    all_val_acc.append(val_acc)\n\n# Print average results over all folds\nprint(f\"Average Training Loss: {np.mean(all_train_loss)}\")\nprint(f\"Average Training Accuracy: {np.mean(all_train_acc)}\")\nprint(f\"Average Validation Loss: {np.mean(all_val_loss)}\")\nprint(f\"Average Validation Accuracy: {np.mean(all_val_acc)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-13T20:05:48.376289Z","iopub.execute_input":"2023-11-13T20:05:48.377108Z","iopub.status.idle":"2023-11-13T20:05:48.772491Z","shell.execute_reply.started":"2023-11-13T20:05:48.377074Z","shell.execute_reply":"2023-11-13T20:05:48.771066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the training","metadata":{}},{"cell_type":"code","source":"# Plot the training\nplt.figure(figsize=(15, 5))\nplt.plot(history.history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\nplt.plot(history.history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\nplt.legend(loc='upper left')\nplt.title('Accuracy')\nplt.grid(alpha=.3)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the model and create submission","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nfrom datetime import datetime\nimport tensorflow as tf\nfrom os.path import basename\n\n# Specify the name of the submission folder\nsubmission_folder = \"SubmissionModel\"\n\n# Save best epoch model\nft_model.save(os.path.join(submission_folder, \"SubmissionModel\"))\n\n# Save the model.py file in the main directory\nwith open(\"model.py\", \"w\") as model_file:\n    model_file.write(\"\"\"\n\nimport os\nimport tensorflow as tf\n\nclass model:\n    def __init__(self, path):\n        self.model = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel'))\n\n    def predict(self, X):\n        X = X/255.0\n        out = self.model.predict(X)\n        out = tf.argmax(out, axis=-1)  # Shape [BS]\n        return out\n\n\"\"\")\n\n# Create an empty metadata file in the main directory\nopen(\"metadata\", \"w\").close()\n\n# Get the current date and time\ncurrent_datetime = datetime.now()\nformatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n\n# Create the zipfile with date and time in the name\nzipfile_name = f\"{submission_folder}_{formatted_datetime}.zip\"\n\nwith zipfile.ZipFile(zipfile_name, 'w') as zip_file:\n    # Add the entire \"SubmissionModel\" folder and its contents to the archive\n    for foldername, subfolders, filenames in os.walk(submission_folder):\n        for filename in filenames:\n            file_path = os.path.join(foldername, filename)\n            arcname = os.path.relpath(file_path, submission_folder)\n\n            if (foldername in [\"SubmissionModel\", \"SubmissionModel/model\", \"SubmissionModel/model/variables\", \"SubmissionModel/variables\"]):\n                continue\n\n            # print(foldername)\n            print(file_path)\n\n            zip_file.write(file_path, arcname)\n\n    # Add other files to the archive (model.py and metadata)\n    zip_file.write(\"model.py\", arcname=\"model.py\")\n    zip_file.write(\"metadata\", arcname=\"metadata\")\n\nprint(zipfile_name)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T21:57:41.773544Z","iopub.execute_input":"2023-11-13T21:57:41.774518Z","iopub.status.idle":"2023-11-13T21:57:44.121968Z","shell.execute_reply.started":"2023-11-13T21:57:41.774475Z","shell.execute_reply":"2023-11-13T21:57:44.121081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{}}]}