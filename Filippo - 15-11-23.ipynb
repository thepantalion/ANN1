{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOr_f850_G6j"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-13T19:09:30.849639Z",
          "iopub.status.busy": "2023-11-13T19:09:30.849257Z",
          "iopub.status.idle": "2023-11-13T19:09:30.856119Z",
          "shell.execute_reply": "2023-11-13T19:09:30.855220Z",
          "shell.execute_reply.started": "2023-11-13T19:09:30.849608Z"
        },
        "id": "lZqUO2x3_G7B",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Fix randomness and hide warnings\n",
        "seed = 42\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "import platform\n",
        "\n",
        "import random\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-13T19:09:31.875522Z",
          "iopub.status.busy": "2023-11-13T19:09:31.875160Z",
          "iopub.status.idle": "2023-11-13T19:09:31.885355Z",
          "shell.execute_reply": "2023-11-13T19:09:31.884438Z",
          "shell.execute_reply.started": "2023-11-13T19:09:31.875493Z"
        },
        "id": "DfwdmJ-J_G7F",
        "outputId": "72c17a33-e83b-45ab-9ec3-b6f463eebbc5",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/thepantalion/Programming/ANN/Homework 1/Filippo - 15-11-23.ipynb Cella 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thepantalion/Programming/ANN/Homework%201/Filippo%20-%2015-11-23.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Import tensorflow\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thepantalion/Programming/ANN/Homework%201/Filippo%20-%2015-11-23.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thepantalion/Programming/ANN/Homework%201/Filippo%20-%2015-11-23.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras \u001b[39mas\u001b[39;00m tfk\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thepantalion/Programming/ANN/Homework%201/Filippo%20-%2015-11-23.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers \u001b[39mas\u001b[39;00m tfkl\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ANN/lib/python3.10/site-packages/tensorflow/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ANN/lib/python3.10/site-packages/tensorflow/python/__init__.py:36\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     38\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaved_model\u001b[39;00m \u001b[39mimport\u001b[39;00m saved_model\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ANN/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:26\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m self_check\n\u001b[1;32m     23\u001b[0m \u001b[39m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m self_check\u001b[39m.\u001b[39;49mpreload_check()\n\u001b[1;32m     28\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m   \u001b[39m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     32\u001b[0m   \u001b[39m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ANN/lib/python3.10/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mCould not find the DLL(s) \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m. TensorFlow requires that these DLLs \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mbe installed in a directory that is named in your \u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39mPATH\u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[39m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[39m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[39m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[39m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[39m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras import mixed_precision\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-13T19:09:32.424413Z",
          "iopub.status.busy": "2023-11-13T19:09:32.423809Z",
          "iopub.status.idle": "2023-11-13T19:09:32.429764Z",
          "shell.execute_reply": "2023-11-13T19:09:32.428818Z",
          "shell.execute_reply.started": "2023-11-13T19:09:32.424379Z"
        },
        "id": "gCwSDDsA_G7J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import other libraries\n",
        "import cv2\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxU6oVTS_G7L"
      },
      "outputs": [],
      "source": [
        "# Setup Mixed Precision\n",
        "\n",
        "# Detect TPY\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "\n",
        "if tpu:\n",
        "  policyConfig = 'mixed_bfloat16'\n",
        "else:\n",
        "  policyConfig = 'mixed_float16'\n",
        "\n",
        "policy = tf.keras.mixed_precision.Policy(policyConfig)\n",
        "# tf.keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rq5_5WD_G7Y"
      },
      "source": [
        "### Load and process the (augmented) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-13T22:19:30.254927Z",
          "iopub.status.busy": "2023-11-13T22:19:30.254521Z",
          "iopub.status.idle": "2023-11-13T22:19:31.403964Z",
          "shell.execute_reply": "2023-11-13T22:19:31.402963Z",
          "shell.execute_reply.started": "2023-11-13T22:19:30.254894Z"
        },
        "id": "sMEyc6xZ_G7Z",
        "outputId": "77c4adb0-5c3f-417b-d567-fa9d64398224",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "file_path = 'augmented_data.npz'\n",
        "loaded_data = np.load(file_path, allow_pickle = True)\n",
        "\n",
        "# Access the 'data' and 'labels' arrays\n",
        "X = loaded_data['data']\n",
        "labels = loaded_data['labels']\n",
        "\n",
        "print(len(X))\n",
        "print(len(labels))\n",
        "\n",
        "X = X.astype('float32') / 255\n",
        "\n",
        "healthy = []\n",
        "unhealthy = []\n",
        "\n",
        "# Iterate through the images to separate them into the proper set\n",
        "for i, image in enumerate(X):\n",
        "    if labels[i] == 'unhealthy':\n",
        "        unhealthy.append(image)\n",
        "    else:\n",
        "        healthy.append(image)\n",
        "\n",
        "healthy = np.shuffle(healthy)\n",
        "unhealthy = np.shuffle(unhealthy)\n",
        "# ChatGPT suggests to shuffle images to avoid batches containing the same images\n",
        "\n",
        "print(len(healthy))\n",
        "print(len(unhealthy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-13T22:19:31.405704Z",
          "iopub.status.busy": "2023-11-13T22:19:31.405438Z",
          "iopub.status.idle": "2023-11-13T22:19:32.435995Z",
          "shell.execute_reply": "2023-11-13T22:19:32.432404Z",
          "shell.execute_reply.started": "2023-11-13T22:19:31.405679Z"
        },
        "id": "QlsL8R6i_G7a",
        "outputId": "bd4b233b-8b8b-49f7-8075-77d5c3a93bd2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Codifica delle etichette 'healthy' e 'unhealthy' in numeri\n",
        "label_dict = {'healthy': 0, 'unhealthy': 1}\n",
        "y = np.array([label_dict[label] for label in labels]) # 0 è [1,0]\n",
        "\n",
        "# Convert labels to one-hot encoding format\n",
        "y = tfk.utils.to_categorical(y,2)\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, random_state=seed, test_size=0.3, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, random_state=seed, test_size=0.3, stratify=y_temp)\n",
        "\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvp4W1NHKwIX",
        "outputId": "61a8cd26-2e6c-4b05-ec57-57bd080288cf"
      },
      "outputs": [],
      "source": [
        "# Define input shape, output shape, batch size, and number of epochs\n",
        "input_shape = X_train.shape[1:]\n",
        "output_shape = y_train.shape[1:]\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "img_height = 96\n",
        "img_width = 96\n",
        "\n",
        "# Print input shape, batch size, and number of epochs\n",
        "print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBFjNOl9_G7c"
      },
      "source": [
        "### Train the (sequential) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-13T22:19:32.446152Z",
          "iopub.status.busy": "2023-11-13T22:19:32.445798Z",
          "iopub.status.idle": "2023-11-13T22:19:32.453975Z",
          "shell.execute_reply": "2023-11-13T22:19:32.453170Z",
          "shell.execute_reply.started": "2023-11-13T22:19:32.446126Z"
        },
        "id": "ZU85HV8x_G7c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True, mode='auto'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Macs with Apple Silicon suffer performance penalties when using the modern Adam optimizer\n",
        "# Detect system specs and select the appropriate optimizer\n",
        "\n",
        "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
        "    optimizer = tf.keras.optimizers.legacy.Adam()\n",
        "else:\n",
        "    optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uC1ygr58Cz_b",
        "outputId": "d284387c-3a42-4434-dfcf-6f90b0d58350"
      },
      "outputs": [],
      "source": [
        "mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='avg',\n",
        ")\n",
        "tfk.utils.plot_model(mobile, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIdxrVyjC29h",
        "outputId": "92d1b783-9774-4472-83ed-e8b1ca0333e5"
      },
      "outputs": [],
      "source": [
        "# Use the supernet as feature extractor, i.e. freeze all its weigths\n",
        "mobile.trainable = False\n",
        "\n",
        "# Create an input layer with shape (96, 96, 3)\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "# Connect MobileNetV2 to the input\n",
        "x = mobile(inputs)\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(2, activation='sigmoid')(x)  # Modifica del numero di neuroni\n",
        "\n",
        "# Create a Model connecting input and output\n",
        "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
        "tl_model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "tl_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EBoq1Lh_G7f"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-11-13T22:19:32.852481Z",
          "iopub.status.busy": "2023-11-13T22:19:32.852170Z"
        },
        "id": "BRW95Xsi_G7f",
        "outputId": "2dc840aa-eeaf-45be-857e-fa0243e1b1b3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tl_history = tl_model.fit(\n",
        "    x=preprocess_input(X_train * 255), #what does it do\n",
        "    y=y_train,\n",
        "    batch_size=128,\n",
        "    epochs=2000,\n",
        "    validation_data=(preprocess_input(X_val * 255), y_val),\n",
        "    callbacks=[tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=200, restore_best_weights=True)]\n",
        ").history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxxP11Qz_G7g"
      },
      "source": [
        "### Validate the (sequential) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEpgLonF_G7g",
        "outputId": "7b563e60-7981-413b-8f30-dcd75b42ca15",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tl_model.save('model_15-11-23')\n",
        "\n",
        "# Define the RandomRotation layer\n",
        "rotation = tf.keras.Sequential([\n",
        "  tfkl.RandomRotation(0.3),\n",
        "])\n",
        "\n",
        "# Define the RandomFlip layer\n",
        "flip = tf.keras.Sequential([\n",
        "  tfkl.RandomFlip(\"vertical\"),\n",
        "])\n",
        "\n",
        "X_test_1 = X_val\n",
        "X_test_2 = rotation(X_val)\n",
        "X_test_3 = flip(X_val)\n",
        "\n",
        "true_labels = y_val\n",
        "\n",
        "# Make predictions on each test set\n",
        "predictions_1 = tl_model.predict(X_test_1)\n",
        "predictions_2 = tl_model.predict(X_test_2)\n",
        "predictions_3 = tl_model.predict(X_test_3)\n",
        "\n",
        "# Assuming binary classification, adjust if needed\n",
        "\n",
        "best_accuracy_1 = 0.0\n",
        "limit_1 = 0.0\n",
        "\n",
        "for limit in range(100):\n",
        "    predicted_labels_1 = (predictions_1 > (limit / 100.0)).astype(int)\n",
        "    accuracy_1 = np.sum(predicted_labels_1 == true_labels) / len(true_labels)\n",
        "\n",
        "    if (accuracy_1 < best_accuracy_1):\n",
        "        continue\n",
        "\n",
        "    best_accuracy_1 = accuracy_1\n",
        "    limit_1 = (limit / 100.0)\n",
        "\n",
        "best_accuracy_2 = 0.0\n",
        "limit_2 = 0.0\n",
        "\n",
        "for limit in range(100):\n",
        "    predicted_labels_2 = (predictions_2 > (limit / 100.0)).astype(int)\n",
        "    accuracy_2 = np.sum(predicted_labels_2 == true_labels) / len(true_labels)\n",
        "\n",
        "    if (accuracy_2 < best_accuracy_2):\n",
        "        continue\n",
        "\n",
        "    best_accuracy_2 = accuracy_2\n",
        "    limit_2 = (limit / 100.0)\n",
        "\n",
        "best_accuracy_3 = 0.0\n",
        "limit_3 = 0.0\n",
        "\n",
        "for limit in range(100):\n",
        "    predicted_labels_3 = (predictions_3 > (limit / 100.0)).astype(int)\n",
        "    accuracy_3 = np.sum(predicted_labels_3 == true_labels) / len(true_labels)\n",
        "\n",
        "    if (accuracy_3 < best_accuracy_3):\n",
        "        continue\n",
        "\n",
        "    best_accuracy_3 = accuracy_3\n",
        "    limit_3 = (limit / 100.0)\n",
        "\n",
        "\n",
        "print(\"Limit on Test Set 1:\", limit_1)\n",
        "print(\"Limit on Test Set 2:\", limit_2)\n",
        "print(\"Limit on Test Set 3:\", limit_3)\n",
        "\n",
        "predicted_labels_1 = (predictions_1 > limit_1).astype(int)\n",
        "predicted_labels_2 = (predictions_2 > limit_2).astype(int)\n",
        "predicted_labels_3 = (predictions_3 > limit_3).astype(int)\n",
        "\n",
        "# Compare the results\n",
        "accuracy_1 = np.sum(predicted_labels_1 == true_labels) / len(true_labels)\n",
        "accuracy_2 = np.sum(predicted_labels_2 == true_labels) / len(true_labels)\n",
        "accuracy_3 = np.sum(predicted_labels_3 == true_labels) / len(true_labels)\n",
        "\n",
        "print(\"Accuracy on Test Set 1:\", accuracy_1)\n",
        "print(\"Accuracy on Test Set 2:\", accuracy_2)\n",
        "print(\"Accuracy on Test Set 3:\", accuracy_3)\n",
        "\n",
        "\n",
        "final_prediction = []\n",
        "\n",
        "for i in range(len(predictions_1)):\n",
        "    healthy_prediction = predicted_labels_1[i] + predicted_labels_2[i] + predicted_labels_3[i]\n",
        "\n",
        "    if (healthy_prediction >= 2):\n",
        "        final_prediction.append([1])\n",
        "\n",
        "    else:\n",
        "        final_prediction.append([0])\n",
        "\n",
        "final_accuracy = np.sum(final_prediction == true_labels) / len(true_labels)\n",
        "\n",
        "print(\"Accuracy of Final Prediction:\", final_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtjbQMl_G7i",
        "outputId": "33e6acb2-8433-4d12-ca31-f6ec2217ba93",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# N predictions to display\n",
        "N = 10\n",
        "\n",
        "# Output the first N predictions for each set\n",
        "print(\"Predictions on Test Set 1:\")\n",
        "print(predictions_1[:N])\n",
        "\n",
        "print(\"\\nPredictions on Test Set 2:\")\n",
        "print(predictions_2[:N])\n",
        "\n",
        "print(\"\\nPredictions on Test Set 3:\")\n",
        "print(predictions_3[:N])\n",
        "\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(np.array(final_prediction[:N]))\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, N, figsize=(15, 5))\n",
        "\n",
        "for i in range(N):\n",
        "    # Plot the original image from X_val\n",
        "    ax = axes[i]\n",
        "    ax.imshow(X_val[i].astype(np.uint8))\n",
        "    ax.set_title(y_val[i])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Xwgq2x_G7i"
      },
      "source": [
        "### Plot the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXgu9jJ_G7j",
        "outputId": "a2ef9720-01de-4ca9-c758-c052672f6922",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Plot the training\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(tl_history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "plt.plot(tl_history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgQij199_G7n"
      },
      "source": [
        "# Save the model and create submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dya0pnRF_G7n",
        "outputId": "53888302-c7e2-4a2a-8df6-1d38dec6733c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from os.path import basename\n",
        "\n",
        "# Specify the name of the submission folder\n",
        "submission_folder = \"SubmissionModel\"\n",
        "\n",
        "# Save best epoch model\n",
        "tl_model.save(os.path.join(submission_folder, \"SubmissionModel\"))\n",
        "\n",
        "# Save the model.py file in the main directory\n",
        "with open(\"model.py\", \"w\") as model_file:\n",
        "    model_file.write(\"\"\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "class model:\n",
        "    def __init__(self, path):\n",
        "        self.model = tf.keras.models.load_model(os.path.join(path, 'SubmissionModel'))\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Define the RandomRotation layer\n",
        "        rotation = tf.keras.Sequential([\n",
        "            tfkl.RandomRotation(0.3),\n",
        "        ])\n",
        "\n",
        "        # Define the RandomFlip layer\n",
        "        flip = tf.keras.Sequential([\n",
        "            tfkl.RandomFlip(\"vertical\"),\n",
        "        ])\n",
        "\n",
        "        limit = tf.constant(0.56, dtype=tf.float32)\n",
        "\n",
        "        X_test_1 = X\n",
        "        X_test_2 = rotation(X) \n",
        "        X_test_3 = flip(X)\n",
        "\n",
        "        out1 = self.model.predict(X_test_1)\n",
        "        out2 = self.model.predict(X_test_2)\n",
        "        out3 = self.model.predict(X_test_3)\n",
        "\n",
        "        out1 = tf.argmax(out1, axis=-1)  # Shape [BS]\n",
        "        out2 = tf.argmax(out2, axis=-1)  # Shape [BS]\n",
        "        out3 = tf.argmax(out3, axis=-1)  # Shape [BS]\n",
        "        \n",
        "        out = []  # Initialize out as an empty list\n",
        "        \n",
        "        for i in range(len(out1)):\n",
        "            hp = 0\n",
        "        \n",
        "            if tf.cast(out1[i], tf.float32) > limit:\n",
        "                hp += 1\n",
        "        \n",
        "            if tf.cast(out2[i], tf.float32) > limit:\n",
        "                hp += 1\n",
        "        \n",
        "            if tf.cast(out3[i], tf.float32) > limit:\n",
        "                hp += 1\n",
        "        \n",
        "            if hp >= 2:\n",
        "                out.append([1])\n",
        "            else:\n",
        "                out.append([0])\n",
        "\n",
        "        out = tf.argmax(out, axis=-1)  # Shape [BS]\n",
        "\n",
        "        return out\n",
        "\"\"\")\n",
        "\n",
        "# Create an empty metadata file in the main directory\n",
        "open(\"metadata\", \"w\").close()\n",
        "\n",
        "# Get the current date and time\n",
        "current_datetime = datetime.now()\n",
        "formatted_datetime = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Create the zipfile with date and time in the name\n",
        "zipfile_name = f\"{submission_folder}_{formatted_datetime}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zipfile_name, 'w') as zip_file:\n",
        "    # Add the entire \"SubmissionModel\" folder and its contents to the archive\n",
        "    for foldername, subfolders, filenames in os.walk(submission_folder):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            arcname = os.path.relpath(file_path, submission_folder)\n",
        "            \n",
        "            if (foldername in [\"SubmissionModel\", \"SubmissionModel/model\", \"SubmissionModel/model/variables\", \"SubmissionModel/variables\"]):\n",
        "                continue\n",
        "        \n",
        "            # print(foldername)\n",
        "            print(file_path)\n",
        "            \n",
        "            zip_file.write(file_path, arcname)\n",
        "\n",
        "    # Add other files to the archive (model.py and metadata)\n",
        "    zip_file.write(\"model.py\", arcname=\"model.py\")\n",
        "    zip_file.write(\"metadata\", arcname=\"metadata\")\n",
        "\n",
        "print(zipfile_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
